{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1aa81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyYAML==5.3.1\n",
    "\n",
    "# conda install -y -c conda-forge pyyaml=5.3.1\n",
    "\n",
    "# ⭐Clone repository\n",
    "# git clone https://github.com/AliaksandrSiarohin/first-order-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate deepfake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82faaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use conda-forge for most scientific packages\n",
    "# conda install -c conda-forge numpy matplotlib pandas scikit-image ipython -y\n",
    "\n",
    "# # Install OpenCV\n",
    "# conda install -c conda-forge opencv -y\n",
    "\n",
    "# # Install plotly and wbdata via pip\n",
    "# pip install plotly wbdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new environment\n",
    "# conda create -n deepfake python=3.12 -y\n",
    "\n",
    "# # Activate it\n",
    "# conda activate deepfake\n",
    "\n",
    "# # Install PyTorch + torchvision + torchaudio (CPU-only for simplicity)\n",
    "# conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "\n",
    "# # Install dlib\n",
    "# conda install -c conda-forge dlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd8a596b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "=== Online-ready Deepfake Demo ===\n",
      "Haar cascade loaded ✅ from haarcascade_frontalface_default.xml\n",
      "Downloading target video...\n",
      "Processing deepfake video...\n",
      "Deepfake video saved: deepfake_demo.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import dlib\n",
    "import urllib.request\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =========================\n",
    "# DEVICE SETUP\n",
    "# =========================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# HAAR CASCADE LOADER\n",
    "# =========================\n",
    "def get_haar_cascade(local_file='haarcascade_frontalface_default.xml'):\n",
    "    \"\"\"Ensure Haar cascade XML is available locally\"\"\"\n",
    "    if not os.path.exists(local_file):\n",
    "        print(\"Haar cascade not found locally. Downloading...\")\n",
    "        url = f'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/{local_file}'\n",
    "        urllib.request.urlretrieve(url, local_file)\n",
    "        print(f\"Haar cascade downloaded to: {local_file}\")\n",
    "    cascade = cv2.CascadeClassifier(local_file)\n",
    "    if cascade.empty():\n",
    "        raise IOError(f\"Failed to load Haar cascade XML at {local_file}\")\n",
    "    print(f\"Haar cascade loaded ✅ from {local_file}\")\n",
    "    return cascade\n",
    "\n",
    "# =========================\n",
    "# FACE PROCESSOR\n",
    "# =========================\n",
    "class FaceProcessor:\n",
    "    def __init__(self):\n",
    "        self.face_cascade = get_haar_cascade()\n",
    "\n",
    "    def detect_faces(self, image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(\n",
    "            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)\n",
    "        )\n",
    "        return faces\n",
    "\n",
    "    def extract_face(self, image, face_coords, padding_ratio=0.2):\n",
    "        x, y, w, h = face_coords\n",
    "        pad = int(w * padding_ratio)\n",
    "        x1 = max(0, x - pad)\n",
    "        y1 = max(0, y - pad)\n",
    "        x2 = min(image.shape[1], x + w + pad)\n",
    "        y2 = min(image.shape[0], y + h + pad)\n",
    "        face = image[y1:y2, x1:x2]\n",
    "        return face, (x1, y1, x2-x1, y2-y1)\n",
    "\n",
    "# =========================\n",
    "# FACE ALIGNER (OPTIONAL)\n",
    "# =========================\n",
    "class FaceAligner:\n",
    "    def __init__(self, predictor_path='shape_predictor_68_face_landmarks.dat'):\n",
    "        if not os.path.exists(predictor_path):\n",
    "            print(f\"Landmark predictor not found: {predictor_path}\")\n",
    "            print(\"Download: http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "            raise FileNotFoundError(predictor_path)\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        self.predictor = dlib.shape_predictor(predictor_path)\n",
    "        \n",
    "    def align_face(self, image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.detector(gray, 1)\n",
    "        if len(faces) == 0:\n",
    "            return None\n",
    "        face = faces[0]\n",
    "        landmarks = self.predictor(gray, face)\n",
    "        left_eye = np.mean([(landmarks.part(i).x, landmarks.part(i).y) for i in range(36,42)], axis=0)\n",
    "        right_eye = np.mean([(landmarks.part(i).x, landmarks.part(i).y) for i in range(42,48)], axis=0)\n",
    "        angle = np.degrees(np.arctan2(right_eye[1]-left_eye[1], right_eye[0]-left_eye[0]))\n",
    "        eyes_center = ((left_eye[0]+right_eye[0])//2, (left_eye[1]+right_eye[1])//2)\n",
    "        M = cv2.getRotationMatrix2D(eyes_center, angle, 1.0)\n",
    "        rotated = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]), flags=cv2.INTER_CUBIC)\n",
    "        return rotated\n",
    "\n",
    "# =========================\n",
    "# AUTOENCODER MODEL\n",
    "# =========================\n",
    "class FaceSwapAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Conv2d(3, 64, 5, 2, 2)\n",
    "        self.enc_conv2 = nn.Conv2d(64, 128, 5, 2, 2)\n",
    "        self.enc_conv3 = nn.Conv2d(128, 256, 5, 2, 2)\n",
    "        self.enc_conv4 = nn.Conv2d(256, 512, 5, 2, 2)\n",
    "        self.bottleneck = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "        # Decoder\n",
    "        self.dec_conv1 = nn.ConvTranspose2d(512, 256, 5, 2, 2, 1)\n",
    "        self.dec_conv2 = nn.ConvTranspose2d(256, 128, 5, 2, 2, 1)\n",
    "        self.dec_conv3 = nn.ConvTranspose2d(128, 64, 5, 2, 2, 1)\n",
    "        self.dec_conv4 = nn.ConvTranspose2d(64, 3, 5, 2, 2, 1)\n",
    "        # BatchNorm\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.norm2 = nn.BatchNorm2d(128)\n",
    "        self.norm3 = nn.BatchNorm2d(256)\n",
    "        self.norm4 = nn.BatchNorm2d(512)\n",
    "        self.dnorm1 = nn.BatchNorm2d(256)\n",
    "        self.dnorm2 = nn.BatchNorm2d(128)\n",
    "        self.dnorm3 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.norm1(self.enc_conv1(x)))\n",
    "        x = F.relu(self.norm2(self.enc_conv2(x)))\n",
    "        x = F.relu(self.norm3(self.enc_conv3(x)))\n",
    "        x = F.relu(self.norm4(self.enc_conv4(x)))\n",
    "        return F.relu(self.bottleneck(x))\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = F.relu(self.dnorm1(self.dec_conv1(x)))\n",
    "        x = F.relu(self.dnorm2(self.dec_conv2(x)))\n",
    "        x = F.relu(self.dnorm3(self.dec_conv3(x)))\n",
    "        return torch.tanh(self.dec_conv4(x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n",
    "\n",
    "# =========================\n",
    "# DEEPFAKE GENERATOR\n",
    "# =========================\n",
    "class DeepfakeGenerator:\n",
    "    def __init__(self):\n",
    "        self.model = FaceSwapAutoencoder().to(device)\n",
    "        self.face_processor = FaceProcessor()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256,256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3,[0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def prepare_face(self, image):\n",
    "        \"\"\"Accepts URL or local path or OpenCV image\"\"\"\n",
    "        if isinstance(image, str):\n",
    "            if image.startswith('http'):\n",
    "                img = Image.open(BytesIO(requests.get(image).content))\n",
    "            else:\n",
    "                img = Image.open(image)\n",
    "            img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "        else:\n",
    "            img = image.copy()\n",
    "        faces = self.face_processor.detect_faces(img)\n",
    "        if len(faces)==0:\n",
    "            raise ValueError(\"No faces detected\")\n",
    "        face_coords = max(faces, key=lambda f: f[2]*f[3])\n",
    "        face_img, _ = self.face_processor.extract_face(img, face_coords)\n",
    "        face_tensor = self.transform(Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))).unsqueeze(0).to(device)\n",
    "        return face_tensor, face_img, face_coords\n",
    "\n",
    "    def blend_faces(self, source_tensor, target_tensor):\n",
    "        \"\"\"Simple blending\"\"\"\n",
    "        return 0.3*source_tensor + 0.7*target_tensor\n",
    "\n",
    "    def tensor_to_image(self, tensor):\n",
    "        tensor = tensor.cpu().detach()*0.5 + 0.5\n",
    "        img = (tensor.permute(1,2,0).numpy()*255).astype(np.uint8)\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    def swap_faces_video(self, source_img, target_video, output_file='deepfake_output.mp4'):\n",
    "        print(\"Processing deepfake video...\")\n",
    "        source_tensor, _, _ = self.prepare_face(source_img)\n",
    "        cap = cv2.VideoCapture(target_video)\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        out = cv2.VideoWriter(output_file, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w,h))\n",
    "        frame_num = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            try:\n",
    "                target_tensor, target_face_img, coords = self.prepare_face(frame)\n",
    "                blended = self.blend_faces(source_tensor, target_tensor)\n",
    "                swapped = self.tensor_to_image(blended[0])\n",
    "                swapped_resized = cv2.resize(swapped, (coords[2], coords[3]))\n",
    "                x, y, w1, h1 = coords\n",
    "                frame[y:y+h1, x:x+w1] = swapped_resized\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            out.write(frame)\n",
    "            frame_num += 1\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Deepfake video saved: {output_file}\")\n",
    "        return output_file\n",
    "\n",
    "# =========================\n",
    "# DEMO EXECUTION\n",
    "# =========================\n",
    "if __name__==\"__main__\":\n",
    "    print(\"=== Online-ready Deepfake Demo ===\")\n",
    "    generator = DeepfakeGenerator()\n",
    "\n",
    "    # Example online files (replace with any URLs)\n",
    "    source_image_url = \"https://raw.githubusercontent.com/opencv/opencv/master/samples/data/lena.jpg\"\n",
    "    target_video_url = \"https://raw.githubusercontent.com/opencv/opencv/master/samples/data/vtest.avi\"\n",
    "\n",
    "    # Download target video if URL\n",
    "    if target_video_url.startswith('http'):\n",
    "        print(\"Downloading target video...\")\n",
    "        r = requests.get(target_video_url)\n",
    "        target_video_path = 'target_video.mp4'\n",
    "        with open(target_video_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    else:\n",
    "        target_video_path = target_video_url\n",
    "\n",
    "    generator.swap_faces_video(source_image_url, target_video_path, 'deepfake_demo.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9fb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "=== Deepfake System Initialized ===\n",
      "Haar cascade loaded ✅ from haarcascade_frontalface_default.xml\n",
      "Source image or target video missing. Generating sample output instead...\n",
      "Sample video created: deepfake_sample.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import dlib\n",
    "import urllib.request\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =========================\n",
    "# DEVICE SETUP\n",
    "# =========================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# HAAR CASCADE SETUP\n",
    "# =========================\n",
    "def get_haar_cascade(local_file='haarcascade_frontalface_default.xml'):\n",
    "    \"\"\"Ensure Haar cascade XML is available locally\"\"\"\n",
    "    if not os.path.exists(local_file):\n",
    "        print(\"Haar cascade not found locally. Downloading...\")\n",
    "        url = f'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/{local_file}'\n",
    "        # Face:\n",
    "# https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "\n",
    "# Eye:\n",
    "# https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye.xml\n",
    "\n",
    "# Smile:\n",
    "# https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_smile.xml\n",
    "\n",
    "# Full body:\n",
    "# https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_fullbody.xml\n",
    "        urllib.request.urlretrieve(url, local_file)\n",
    "        print(f\"Haar cascade downloaded to: {local_file}\")\n",
    "    cascade = cv2.CascadeClassifier(local_file)\n",
    "    if cascade.empty():\n",
    "        raise IOError(f\"Failed to load Haar cascade XML at {local_file}\")\n",
    "    print(f\"Haar cascade loaded ✅ from {local_file}\")\n",
    "    return cascade\n",
    "\n",
    "# =========================\n",
    "# FACE PROCESSOR\n",
    "# =========================\n",
    "class FaceProcessor:\n",
    "    def __init__(self):\n",
    "        self.face_cascade = get_haar_cascade()\n",
    "\n",
    "    def detect_faces(self, image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(\n",
    "            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30,30)\n",
    "        )\n",
    "        return faces\n",
    "\n",
    "    def extract_face(self, image, face_coords):\n",
    "        x, y, w, h = face_coords\n",
    "        padding = int(w * 0.2)\n",
    "        x1 = max(0, x - padding)\n",
    "        y1 = max(0, y - padding)\n",
    "        x2 = min(image.shape[1], x + w + padding)\n",
    "        y2 = min(image.shape[0], y + h + padding)\n",
    "        return image[y1:y2, x1:x2], (x1, y1, x2-x1, y2-y1)\n",
    "\n",
    "# =========================\n",
    "# OPTIONAL: FACE ALIGNER\n",
    "# =========================\n",
    "class FaceAligner:\n",
    "    def __init__(self, predictor_path='shape_predictor_68_face_landmarks.dat'):\n",
    "        if not os.path.exists(predictor_path):\n",
    "            print(f\"Landmark predictor not found: {predictor_path}\")\n",
    "            print(\"Download from: http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "            raise FileNotFoundError(predictor_path)\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        self.predictor = dlib.shape_predictor(predictor_path)\n",
    "    \n",
    "    def align_face(self, image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.detector(gray, 1)\n",
    "        if len(faces) == 0:\n",
    "            return None\n",
    "        face = faces[0]\n",
    "        landmarks = self.predictor(gray, face)\n",
    "        left_eye = np.mean([(landmarks.part(i).x, landmarks.part(i).y) for i in range(36,42)], axis=0)\n",
    "        right_eye = np.mean([(landmarks.part(i).x, landmarks.part(i).y) for i in range(42,48)], axis=0)\n",
    "        angle = np.degrees(np.arctan2(right_eye[1]-left_eye[1], right_eye[0]-left_eye[0]))\n",
    "        eyes_center = ((left_eye[0]+right_eye[0])//2, (left_eye[1]+right_eye[1])//2)\n",
    "        M = cv2.getRotationMatrix2D(eyes_center, angle, 1.0)\n",
    "        return cv2.warpAffine(image, M, (image.shape[1], image.shape[0]), flags=cv2.INTER_CUBIC)\n",
    "\n",
    "# =========================\n",
    "# AUTOENCODER MODEL\n",
    "# =========================\n",
    "class FaceSwapAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Conv2d(3, 64, 5, 2, 2)\n",
    "        self.enc_conv2 = nn.Conv2d(64, 128, 5, 2, 2)\n",
    "        self.enc_conv3 = nn.Conv2d(128, 256, 5, 2, 2)\n",
    "        self.enc_conv4 = nn.Conv2d(256, 512, 5, 2, 2)\n",
    "        self.bottleneck = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "        # Decoder\n",
    "        self.dec_conv1 = nn.ConvTranspose2d(512, 256, 5, 2, 2, 1)\n",
    "        self.dec_conv2 = nn.ConvTranspose2d(256, 128, 5, 2, 2, 1)\n",
    "        self.dec_conv3 = nn.ConvTranspose2d(128, 64, 5, 2, 2, 1)\n",
    "        self.dec_conv4 = nn.ConvTranspose2d(64, 3, 5, 2, 2, 1)\n",
    "        # BatchNorm\n",
    "        self.norm1, self.norm2, self.norm3, self.norm4 = nn.BatchNorm2d(64), nn.BatchNorm2d(128), nn.BatchNorm2d(256), nn.BatchNorm2d(512)\n",
    "        self.dnorm1, self.dnorm2, self.dnorm3 = nn.BatchNorm2d(256), nn.BatchNorm2d(128), nn.BatchNorm2d(64)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.norm1(self.enc_conv1(x)))\n",
    "        x = F.relu(self.norm2(self.enc_conv2(x)))\n",
    "        x = F.relu(self.norm3(self.enc_conv3(x)))\n",
    "        x = F.relu(self.norm4(self.enc_conv4(x)))\n",
    "        return F.relu(self.bottleneck(x))\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = F.relu(self.dnorm1(self.dec_conv1(x)))\n",
    "        x = F.relu(self.dnorm2(self.dec_conv2(x)))\n",
    "        x = F.relu(self.dnorm3(self.dec_conv3(x)))\n",
    "        return torch.tanh(self.dec_conv4(x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n",
    "\n",
    "# =========================\n",
    "# DEEPFAKE GENERATOR\n",
    "# =========================\n",
    "class DeepfakeGenerator:\n",
    "    def __init__(self):\n",
    "        self.model = FaceSwapAutoencoder().to(device)\n",
    "        self.face_processor = FaceProcessor()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def prepare_face(self, image):\n",
    "        \"\"\"Load image and convert to tensor\"\"\"\n",
    "        if isinstance(image, str):\n",
    "            if image.startswith('http'):\n",
    "                response = requests.get(image)\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "            else:\n",
    "                img = Image.open(image)\n",
    "            img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "        else:\n",
    "            img = image.copy()\n",
    "        faces = self.face_processor.detect_faces(img)\n",
    "        if len(faces) == 0:\n",
    "            raise ValueError(\"No faces detected in image/video frame\")\n",
    "        face_coords = max(faces, key=lambda f: f[2]*f[3])\n",
    "        face_img, _ = self.face_processor.extract_face(img, face_coords)\n",
    "        face_tensor = self.transform(Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))).unsqueeze(0).to(device)\n",
    "        return face_tensor, face_img, face_coords\n",
    "\n",
    "    def simple_blend(self, source_tensor, target_tensor):\n",
    "        \"\"\"Demo blending\"\"\"\n",
    "        return 0.3*source_tensor + 0.7*target_tensor\n",
    "\n",
    "    def tensor_to_image(self, tensor):\n",
    "        tensor = tensor.cpu().detach()*0.5 + 0.5\n",
    "        img = (tensor.permute(1,2,0).numpy()*255).astype(np.uint8)\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    def swap_faces_video(self, source_image, target_video, output='deepfake_output.mp4'):\n",
    "        \"\"\"Process video and swap faces\"\"\"\n",
    "        source_tensor, _, _ = self.prepare_face(source_image)\n",
    "        cap = cv2.VideoCapture(target_video)\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        out = cv2.VideoWriter(output, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w,h))\n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            try:\n",
    "                target_tensor, target_face_img, coords = self.prepare_face(frame)\n",
    "                blended = self.simple_blend(source_tensor, target_tensor)\n",
    "                swapped = self.tensor_to_image(blended[0])\n",
    "                swapped_resized = cv2.resize(swapped, (coords[2], coords[3]))\n",
    "                x,y,w1,h1 = coords\n",
    "                frame[y:y+h1, x:x+w1] = swapped_resized\n",
    "            except Exception:\n",
    "                pass\n",
    "            out.write(frame)\n",
    "            frame_count += 1\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"✅ Deepfake saved: {output}\")\n",
    "\n",
    "# =========================\n",
    "# DEMO / SAMPLE OUTPUT\n",
    "# =========================\n",
    "def create_sample_video(output='deepfake_sample.mp4'):\n",
    "    \"\"\"Generates a sample video with text if source files missing\"\"\"\n",
    "    width, height, fps, duration = 640,480,30,5\n",
    "    out = cv2.VideoWriter(output, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width,height))\n",
    "    for i in range(fps*duration):\n",
    "        frame = np.zeros((height,width,3),dtype=np.uint8)\n",
    "        cv2.putText(frame,\"Deepfake Output Sample\",(50,200),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,255),2)\n",
    "        cv2.putText(frame,f\"Frame {i+1}\",(50,250),cv2.FONT_HERSHEY_SIMPLEX,0.7,(255,255,255),2)\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "    print(f\"Sample video created: {output}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__==\"__main__\":\n",
    "    print(\"=== Deepfake System Initialized ===\")\n",
    "    generator = DeepfakeGenerator()\n",
    "\n",
    "    # File paths\n",
    "    source_image = \"source_face.jpg\"\n",
    "    target_video = \"target_video.mp4\"\n",
    "\n",
    "    # Check files\n",
    "    if not os.path.exists(source_image) or not os.path.exists(target_video):\n",
    "        print(\"Source image or target video missing. Generating sample output instead...\")\n",
    "        create_sample_video()\n",
    "    else:\n",
    "        generator.swap_faces_video(source_image, target_video, \"deepfake_demo_output.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a811c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
