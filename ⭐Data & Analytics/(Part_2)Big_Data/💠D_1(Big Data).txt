ğŸŒŸ Big Data & Hadoop Complete Notes ğŸš€

---

â­ Chapter 1: What is Big Data?  
ğŸ’¡ Definition: Extremely large volume of data  
- Size can be GB, TB, PB, depending on context  
- Characteristics: Volume, Velocity, Variety  

ğŸ”¹ Vs of Big Data:  
- Volume: 10GB, 100GB, 100TB  
- Velocity: 1 min, 1 hour, 1 day  
- Variety: Structured, Semi-structured, Unstructured  

ğŸ”¹ Challenges of Big Data:  
- Data loss âŒ  
- Computational efficiency âš¡  
- Storage & cost ğŸ’°  

---

â­ Chapter 2: Big Data Workshop - Problems  
ğŸ”¹ Sample Data: Daily closing prices of many stock symbols for many years (~1TB)  
- Task: Find maximum closing price per stock  

ğŸ”¹ Storage & Computation Challenges:  
- Total processing time: 4 hours â±  
  - Data transfer: 2.5 hours  
  - Computation: 1 hour  
  - Network latency: 30 min  

ğŸ”¹ Solution Approach:  
- Split 1TB into 100 equal parts âœ…  
- Process in parallel (reduce read + compute time) âš¡  
- Store data closer to computation  
- Data loss protection ğŸ›¡  
- Distributed computing  

---

â­ Chapter 3: Hadoop Framework  
ğŸ”¹ Challenges in distributed computing: managing, processing, storing  
- HDFS: Distributed File System ğŸ“‚  
- MapReduce: Distributed computation ğŸ”„  
- YARN: Resource management ğŸ–¥  

---

â­ Chapter 4: Hadoop Ecosystem  
ğŸ”¹ Data Warehouse:  
- Sources: databases, flat files, applications  
- ETL processing ğŸ—  
- Reporting ğŸ“Š  
- OLAP analysis ğŸ”  
- Ad-hoc queries ğŸ“  

ğŸ”¹ Hadoop Ecosystem vs Data Lake:  
- Hadoop: structured + semi-structured data  
- Data Lake: store raw data in any format ğŸŒŠ  

---

â­ Chapter 5: Hadoop Data Load Help  
- Storage optimization  
- Efficient block distribution  
- Data replication  

---

â­ Chapter 6: NoSQL (New Data Types)  
- SQL not-relational database âŒ  
- Stores data dynamically (schema-less)  
- Uses document or key-value storage ğŸ“„  

---

â­ Chapter 7: New Database Types  

ğŸ”¹ Key-Value Stores:  
- Example: Redis, Riak, Amazon DynamoDB  
- Structure: Box -> StoreBox  
- Fast lookup by key ğŸ”‘  

ğŸ”¹ Document Stores:  
- Example: MongoDB, CouchDB, Firebase  
- Structure: Tree ğŸŒ³  
- Store data with key-value pairs  

ğŸ”¹ Wide-Column Stores:  
- Example: SAP HANA, Apache HBase, Cassandra  
- Column-oriented storage  
- Fast analytics on large datasets  

ğŸ”¹ Graph Databases:  
- Example: Neo4j, Amazon Neptune  
- Data in nodes + relationships  
- Ideal for social networks, recommendations ğŸŒ  

---

â­ Chapter 8: Hadoop Distributions  

1ï¸âƒ£ Hadoop Distributions (software only):  
- Apache Hadoop (core + open-source)  
- IBM Open Platform  
- Popular: Cloudera, Hortonworks  
- Less popular: MapR  

2ï¸âƒ£ Hadoop Applications (software + hardware):  
- Dell EMC, Teradata, HP, Oracle, NetApp  

3ï¸âƒ£ Managed Hadoop (Cloud):  
- Amazon EMR, HDInsight, Google Cloud Dataproc  
- IBM BigInsights, Qubole  

Basic Components:  
- Apache Hadoop: HDFS + YARN  
- Hortonworks: Cluster management  
- Cloudera: File management  

---

â­ Chapter 9: Setting Up Environment  
- GitHub: https://github.com/big-data-europe/docker-hive  
- Command: `docker-compose up -d`  

---

â­ Chapter 10: HDFS  

ğŸ”¹ Why a new file system?  
- Efficient storage & block distribution  
- Manage metadata, permissions, and security  
- Cost-effective & scalable  

ğŸ”¹ Local FS vs HDFS:  
- HDFS: distributed, blocks stored across nodes  

ğŸ”¹ Basic HDFS commands:  
```bash
docker ps
docker exec -it docker-hive-hive-server-1 /bin/bash
ls, mkdir, cp, mv, rm
docker exec -it docker-hive-namenode-1 /bin/bash
hadoop fs -ls /
hadoop fs -mkdir /hadoop-test1
docker cp <local_file> <container>:/tmp/
hadoop fs -copyFromLocal /tmp/RandomCountries.txt /hadoop-test1/
hadoop fs -cat /hadoop-test1/RandomCountries.txt
hdfs dfs -cp /hadoop-test1/RandomCountries.txt /hadoop.txt2/
hdfs dfs -mv /hadoop-test1/RandomCountries.txt /hadoop.txt
hdfs dfs -chmod 777 /hadoop.txt2/RandomCountries.txt
hdfs fsck /hadoop.txt2 -files -blocks -locations
hdfs dfs -setrep -w 1 /hadoop.txt2/RandomCountries.txt


â­ Chapter 11: HDFS Architecture

ğŸ”¹ Physical storage structure:
Master node â†’ metadata + block locations
DataNodes â†’ store actual blocks
Standby nodes â†’ failover
Cluster hierarchy: Node â†’ Rack â†’ Cluster â†’ Data Center ğŸ¢

â­ Chapter 12: MapReduce

ğŸ”¹ Concept: Distributed programming model for processing large datasets
Input â†’ Mappers â†’ Key-Value Pairs â†’ Reducers â†’ Output
Not a programming language; Hadoop provides implementation

â­ Chapter 13: MapReduce Components

Example problem: Max stock closing price per symbol
Algorithm: Read line â†’ Extract symbol & price â†’ Compare â†’ Update max
Distributed computing using: Block â†’ Input Split â†’ Map â†’ Reduce
Combiner reduces intermediate data for efficiency

â­ Chapter 14: MapReduce Hands-On

docker exec -it datanode /bin/bash
mkdir WordCount

â­ Chapter 15: Hive

Hive Architecture: Hadoop + MapReduce
Structured data storage
Query languages: HiveQL / SQL
Tables: Internal / External
Supports batch processing & analytics

================================================     â­    ================================================
================================================ [Advanced]================================================
================================================       â­  ================================================

ğŸŒŸ BIG DATA & HADOOP: THE ULTIMATE ONE-PAGE REFERENCE ğŸš€

========================================================
ğŸ“Š BIG DATA THEORY ğŸ“Š
========================================================

ğŸ“ˆ THE 3 V's OF BIG DATA:
â”œâ”€ Volume: 10GB â†’ 100GB â†’ 1TB â†’ 1PB
â”œâ”€ Velocity: Real-time â†’ Daily â†’ Batch
â””â”€ Variety: Structured â†’ Semi-structured â†’ Unstructured

ğŸ’¾ DATA TYPES:
â”œâ”€ Structured: SQL Databases (MySQL, PostgreSQL) ğŸ›ï¸
â”œâ”€ Semi-structured: JSON, XML, CSV ğŸ“„
â””â”€ Unstructured: Images, Videos, Text files ğŸ¥ğŸ“¸

ğŸ”¥ BIG DATA CHALLENGES:
â”œâ”€ Storage Cost: $$$ per TB ğŸ’°
â”œâ”€ Processing Time: Hours â†’ Days â±ï¸
â”œâ”€ Data Loss: Need fault tolerance âŒ
â””â”€ Data Movement: Network bottlenecks ğŸŒ

========================================================
ğŸ˜ HADOOP ECOSYSTEM ğŸ˜
========================================================

ğŸ—ï¸ CORE COMPONENTS:
â”œâ”€ HDFS (Hadoop Distributed File System) ğŸ“‚
â”‚  â”œâ”€ NameNode: Master (metadata) ğŸ‘‘
â”‚  â””â”€ DataNode: Slave (data storage) ğŸ“¦
â”œâ”€ MapReduce: Processing framework ğŸ”„
â””â”€ YARN: Resource manager ğŸ–¥ï¸

ğŸ”§ ECOSYSTEM TOOLS:
â”œâ”€ Hive: SQL interface ğŸ
â”œâ”€ Spark: Fast processing âš¡
â”œâ”€ HBase: NoSQL database ğŸ—ƒï¸
â”œâ”€ Pig: Data flow scripting ğŸ·
â”œâ”€ Sqoop: SQL â†” Hadoop ğŸ”„
â””â”€ Flume: Log aggregation ğŸ“

ğŸ¢ HADOOP DISTRIBUTIONS:
â”œâ”€ Apache: Pure open-source ğŸ†“
â”œâ”€ Cloudera: CDH distribution ğŸ¢
â”œâ”€ Hortonworks: HDP distribution ğŸ­
â””â”€ Cloud: EMR (AWS), Dataproc (GCP) â˜ï¸

========================================================
ğŸ“‚ HDFS COMMAND CHEAT SHEET ğŸ“‚
========================================================

ğŸš€ FILE OPERATIONS:
â”œâ”€ List: hadoop fs -ls /path ğŸ“‹
â”œâ”€ Create Dir: hadoop fs -mkdir /path ğŸ“
â”œâ”€ Copy Local â†’ HDFS: hadoop fs -copyFromLocal local.txt /hdfs/path ğŸ“¤
â”œâ”€ Copy HDFS â†’ Local: hadoop fs -copyToLocal /hdfs/path local.txt ğŸ“¥
â”œâ”€ Move: hadoop fs -mv /src /dest ğŸ”„
â”œâ”€ Remove: hadoop fs -rm /file âŒ
â””â”€ View: hadoop fs -cat /file ğŸ‘ï¸

âš™ï¸ ADMIN COMMANDS:
â”œâ”€ Check Health: hdfs fsck /path -files -blocks -locations ğŸ©º
â”œâ”€ Set Replication: hdfs dfs -setrep -w 3 /file ğŸ”¢
â”œâ”€ Permissions: hdfs dfs -chmod 755 /file ğŸ”
â”œâ”€ Disk Usage: hdfs dfs -du -h /path ğŸ’½
â””â”€ Count: hdfs dfs -count /path ğŸ§®

========================================================
âš¡ MAPREDUCE FLOW âš¡
========================================================

ğŸ“Š PROCESSING PIPELINE:
Input Data â†’ Split â†’ Mapper â†’ Shuffle â†’ Reducer â†’ Output

ğŸ”§ EXAMPLE: MAX STOCK PRICE ğŸ“ˆ
Input: Stock data (1TB)
1ï¸âƒ£ Split into 100 parts (10GB each)
2ï¸âƒ£ Mappers find local max per stock
3ï¸âƒ£ Shuffle groups by stock symbol
4ï¸âƒ£ Reducers compute global max
5ï¸âƒ£ Output to HDFS

â±ï¸ TIME SAVINGS:
Without Parallel: 4 hours ğŸ¢
With MapReduce: 30 minutes âš¡

========================================================
ğŸ HIVE QUICK REFERENCE ğŸ
========================================================

ğŸ“ BASIC COMMANDS:
â”œâ”€ Start: hive ğŸš€
â”œâ”€ Show DBs: SHOW DATABASES; ğŸ“Š
â”œâ”€ Use DB: USE database; ğŸ¯
â”œâ”€ Show Tables: SHOW TABLES; ğŸ“‹
â””â”€ Describe: DESCRIBE table; ğŸ“–

ğŸ—ï¸ CREATE TABLE:
CREATE TABLE stocks (
  symbol STRING,
  date DATE,
  price DECIMAL(10,2)
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';

ğŸ“¥ LOAD DATA:
LOAD DATA INPATH '/hdfs/path/data.csv' INTO TABLE stocks;

ğŸ” QUERIES:
â”œâ”€ Basic: SELECT * FROM stocks LIMIT 10; ğŸ”
â”œâ”€ Aggregate: SELECT symbol, MAX(price) FROM stocks GROUP BY symbol; ğŸ“ˆ
â”œâ”€ Filter: SELECT * FROM stocks WHERE price > 100 AND date > '2024-01-01'; ğŸ¯
â””â”€ Join: SELECT a.*, b.company FROM stocks a JOIN companies b ON a.symbol = b.symbol; ğŸ¤

========================================================
ğŸ³ DOCKER HADOOP SETUP ğŸ³
========================================================

ğŸš€ QUICK START:
1. git clone https://github.com/big-data-europe/docker-hive
2. cd docker-hive
3. docker-compose up -d

ğŸ” CONTAINER NAMES:
â”œâ”€ NameNode: docker-hive-namenode-1
â”œâ”€ DataNode: docker-hive-datanode-1
â”œâ”€ Hive: docker-hive-hive-server-1
â””â”€ Presto: docker-hive-presto-coordinator-1

ğŸ“ FILE COPY:
docker cp local_file.txt container_name:/tmp/

ğŸ–¥ï¸ SHELL ACCESS:
docker exec -it container_name /bin/bash

========================================================
ğŸ’¡ PRO TIPS & BEST PRACTICES ğŸ’¡
========================================================

ğŸ¯ PERFORMANCE:
â”œâ”€ Keep data near computation (DataNode placement) ğŸƒâ€â™‚ï¸
â”œâ”€ Use appropriate block size (128MB default) ğŸ“
â”œâ”€ Enable compression for text files ğŸ—œï¸
â”œâ”€ Use combiners in MapReduce to reduce shuffle data ğŸ“‰
â””â”€ Partition and bucket tables in Hive for faster queries âš¡

ğŸ›¡ï¸ RELIABILITY:
â”œâ”€ Set replication factor â‰¥ 3 in production ğŸ”„
â”œâ”€ Regular fsck checks for HDFS health ğŸ©º
â”œâ”€ Backup NameNode metadata regularly ğŸ’¾
â””â”€ Monitor disk space on DataNodes ğŸ“Š

ğŸ” SECURITY:
â”œâ”€ Set proper file permissions (chmod) ğŸ”
â”œâ”€ Use Kerberos for authentication in production ğŸ›¡ï¸
â”œâ”€ Enable HDFS encryption for sensitive data ğŸ”’
â””â”€ Regular security audits ğŸ”

ğŸ’° COST OPTIMIZATION:
â”œâ”€ Use erasure coding instead of replication for cold data ğŸ’¸
â”œâ”€ Archive old data to cheaper storage (S3 Glacier) ğŸ“¦
â”œâ”€ Use spot instances for batch processing in cloud â˜ï¸
â””â”€ Monitor and kill unused jobs â±ï¸

========================================================
ğŸ“Š REAL-WORLD USE CASES ğŸ“Š
========================================================

ğŸ¦ FINANCE:
â”œâ”€ Fraud detection in transaction logs ğŸ”
â”œâ”€ Risk modeling with historical data ğŸ“ˆ
â”œâ”€ Algorithmic trading analysis ğŸ“Š

ğŸ›’ RETAIL:
â”œâ”€ Customer behavior analysis ğŸ‘¥
â”œâ”€ Recommendation systems ğŸ¯
â”œâ”€ Inventory optimization ğŸ“¦

ğŸ¥ HEALTHCARE:
â”œâ”€ Genomic data processing ğŸ§¬
â”œâ”€ Patient record analysis ğŸ“‹
â”œâ”€ Medical imaging storage ğŸ“¸

ğŸŒ SOCIAL MEDIA:
â”œâ”€ Sentiment analysis on posts ğŸ’¬
â”œâ”€ Friend recommendations ğŸ¤
â”œâ”€ Ad targeting optimization ğŸ¯

========================================================
ğŸš¨ TROUBLESHOOTING GUIDE ğŸš¨
========================================================

âŒ COMMON ISSUES:
â”œâ”€ "No such file or directory": Check path exists with -ls â“
â”œâ”€ "Permission denied": Use chmod or check user permissions ğŸ”
â”œâ”€ "Disk full": Check DataNode disk space ğŸ’½
â”œâ”€ "Connection refused": Check if services are running ğŸ”Œ
â””â”€ "Slow queries": Check partitions and indexing â±ï¸

ğŸ”§ QUICK FIXES:
â”œâ”€ Restart services: docker-compose restart ğŸ”„
â”œâ”€ Check logs: docker logs container_name ğŸ“‹
â”œâ”€ Increase memory: Set HADOOP_HEAPSIZE in env ğŸ§ 
â””â”€ Check network: ping between containers ğŸŒ

========================================================
ğŸ¯ LEARNING PATH ğŸ¯
========================================================

ğŸ“š RECOMMENDED ORDER:
1. HDFS Basics â†’ File operations ğŸ“‚
2. MapReduce â†’ Processing logic ğŸ”„
3. Hive â†’ SQL interface ğŸ
4. Spark â†’ Faster processing âš¡
5. Real projects â†’ Portfolio building ğŸ—ï¸

ğŸ† SUCCESS METRICS:
â”œâ”€ Can process 1GB dataset in <5 minutes âš¡
â”œâ”€ Build 3 end-to-end data pipelines ğŸ—ï¸
â”œâ”€ Optimize query from 10min to <1min ğŸ¯
â””â”€ Deploy to cloud (AWS EMR/GCP Dataproc) â˜ï¸

âœ¨ REMEMBER:
"Big Data is not about the size, 
it's about what you DO with it!" ğŸš€